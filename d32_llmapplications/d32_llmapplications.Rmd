---
title: "Use Cases of Large Language Models <br> `r emo::ji('robot')`"
author: "S. Mason Garrison"
output:
  xaringan::moon_reader:
    css: "../slides.css"
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
      slideNumberFormat: ""
---

```{r child = "../setup.Rmd"}
```

```{r packages, echo = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
# Remember to compile
# xaringan::inf_mr(cast_from = "..")
```

class: middle

# Learning Goals

---

## Learning Goals

By the end of this session, you will be able to...

- Apply LLMs to text classification, sentiment analysis, and NER tasks
- Implement text generation using Markov chains and sequence modeling
- Build question answering systems through text matching and retrieval
- Evaluate different NLP techniques in R for practical data science problems

---



class: middle

# Use Cases of Large Language Models

---

# Roadmap

1. Text Understanding and Analysis
2. Text Generation and Transformation
3. Question Answering and Information Retrieval


---

class: middle

# 1. Text Understanding and Analysis

---

## Text Understanding and Analysis

- Extract insights from text data
- Applications:
  - Text classification
  - Named entity recognition
  - Text similarity and clustering
  - Anomaly detection

---

## Text Classification 

- Categorize text into predefined classes
- Applications:
  - Sentiment analysis 
  - Topic classification
  - Intent detection in chatbots
- Fine-tuned LLMs like BERT achieve state-of-the-art results
  - [Tensorflow has really solid tutorials](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)

---

## Named Entity Recognition

- Identify and classify named entities in within unstructured text.
- Use cases:
  - Information extraction
  - Building knowledge graphs
  - Enhancing search capabilities
- Models like [SpaCy](https://spacy.io/) and [BERT](https://arxiv.org/abs/1810.04805) can be fine-tuned for NER

---

## Text Similarity and Clustering

- Measure semantic similarity between texts
- Use cases:
  - Document de-duplication
  - Content recommendation systems
- Sentence transformers like [SBERT](https://sbert.net/) are effective for this

---

## Anomaly Detection in Text Data

- Identify unusual patterns or outliers in text
- Use cases:
  - Fraud detection in financial documents
  - Identifying errors in medical records
- Autoencoder architectures with LLMs can be used
- **Check out this [Paper that talks more about this](https://arxiv.org/abs/2211.13900)**

---

## R Example: Text Classification (Sentiment Analysis)

Let us perform sentiment analysis on the opening lines of Jane Austen's novels.

```{r echo=FALSE}
library(janeaustenr)
library(dplyr)
library(stringr)

tidy_books_openinglines <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
    filter(linenumber < 100& chapter == 1 & text != "" & !text %in% c("CHAPTER 1","CHAPTER I","Chapter 1")) %>% 
  select(book, text) %>% # select columns
  slice_head(n = 2)%>% # slice the first sentence by book 
# concatenate the text by book
  summarise(text = paste(text, collapse = " ")) %>% 
  mutate(book = str_remove(book, "_")) %>% 
  ungroup()

tidy_books_openinglines %>% 
  knitr::kable()
#  ungroup() %>%
# unnest_tokens(word, text)

tidy_books_chap1 <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
    filter(chapter == 1 & linenumber < 50 & text != "" & !text %in% c("CHAPTER 1","CHAPTER I","Chapter 1")) %>% 
  select(book, text) %>% # select columns
#  slice_head(n = 2)%>% # slice the first sentence by book 
# concatenate the text by book
  summarise(text = paste(text, collapse = " ")) %>% 
  mutate(book = str_remove(book, "_")) %>% 
  ungroup()
tidy_books_openinglines_chap2 <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
    filter(chapter == 2 & text != "" & !text %in% c("CHAPTER 2","CHAPTER II","Chapter 2")) %>% 
  select(book, text) %>% # select columns
  slice_head(n = 2)%>% # slice the first sentence by book 
# concatenate the text by book
  summarise(text = paste(text, collapse = " ")) %>% 
  mutate(book = str_remove(book, "_")) %>% 
  ungroup()

```

---

## Tokenize and Perform Sentiment Analysis

.midi[
```{r}

library(tidytext)
library(dplyr)
library(ggplot2)

sentiment_scores <- tidy_books_openinglines %>%
  unnest_tokens(word, text) %>%
  inner_join(get_sentiments("bing")) %>%
  count(sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment_score = positive - negative)


sentiment_scores
```
]

---

## Visualize Sentiment Analysis Results

.pull-left[

```{r,echo=FALSE,fig.align='center', out.width="90%"}
tidy_books_openinglines  %>%
  unnest_tokens(word, text) %>%
  inner_join(get_sentiments("bing")) %>%
  ggplot(aes(x = sentiment)) +
  geom_bar(fill = "#0078D7", color = "black", width = 0.7) + # Darker blue bars with black borders
  coord_flip() +
  theme_minimal(base_size = 18) + # Larger text for better readability
  theme(
    panel.background = element_rect(fill = "white", color = NA), # Match background color to slide
    plot.background = element_rect(fill = "white", color = NA),
    text = element_text(color = "black"), # Match font color to slide
    plot.title = element_text(face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "gray40", linewidth = 0.5), # Use `linewidth` for gridline thickness
    panel.grid.minor = element_line(color = "gray70", linewidth = 0.25)  # Use `linewidth` for minor gridlines
  ) +
  labs(
    title = "Overall Vibes",
    x = "",
    y = "Count"
  )

```
]
.pull-right[

```{r, echo=FALSE, warning=FALSE, fig.align='center', out.width="90%"}

#install.packages("tidyverse")
#install.packages("tidytext")
#install.packages("ggwordcloud")

library(tidyverse)
library(tidytext)
library(ggwordcloud)
# Tokenize and join with sentiment lexicon
word_sentiments <- tidy_books_openinglines%>%
  unnest_tokens(word, text) %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)

# Assign colors based on sentiment
word_sentiments <- word_sentiments %>%
  mutate(color = ifelse(sentiment == "positive", "#34c9f4", "#f35b33"))

# Generate Word Cloud
ggplot(word_sentiments, aes(label = word, size = n, color = sentiment)) +
  geom_text_wordcloud_area() +
  scale_color_manual(values = c("positive" = "#34c9f4", "negative" = "#f35b33")) +
  scale_size_area(max_size = 40, trans = power_trans(1/.7)) +  theme_minimal()

```

]

---


## Visualize Sentiment Analysis Results (Chapter 1)

```{r, echo=FALSE, warning=FALSE, fig.align='center', out.width="85%"}

#install.packages("tidyverse")
#install.packages("tidytext")
#install.packages("ggwordcloud")

library(tidyverse)
library(tidytext)
library(ggwordcloud)
# Tokenize and join with sentiment lexicon
word_sentiments <- tidy_books_chap1 %>%
  unnest_tokens(word, text) %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)

# Assign colors based on sentiment
word_sentiments <- word_sentiments %>%
  mutate(color = ifelse(sentiment == "positive", "#34c9f4", "#f35b33"))

# Generate Word Cloud
ggplot(word_sentiments, aes(label = word, size = n, color = sentiment)) +
  geom_text_wordcloud(rm_outside =TRUE) +
  scale_color_manual(values = c("positive" = "#34c9f4", "negative" = "#f35b33")) +
  scale_size_area(max_size = 20) +
  theme_minimal()


```

<!-- Note that MISS is still the largest word, but the word cloud is more balanced -->
---

class: middle

# 2. Text Generation and Transformation

---

## Text Generation

.pull-left[
- LLMs can generate human-like text on any topic
- Applications:
  - Automated content creation 
  - Chatbots and conversational AI
  - Code generation
- Examples: 
  - GPT-3 for natural language generation
  - GitHub Copilot for code completion
]

.pull-right[
```{r echo=FALSE, out.width="90%",error=TRUE}
knitr::include_graphics("img/An image illustrating automated content creation.png")
```
]

---

## Summarization

.pull-left[
- Condense long documents into brief summaries
- Use cases:
  - Summarizing research papers
  - Creating article abstracts
  - Distilling key points from large datasets
- Models like BART and T5 specialize in summarization tasks
]

---

## Machine Translation

- Translate text between languages
- Applications:
  - Localization of content
  - Cross-lingual information retrieval
- Models like mT5 specialize in multilingual tasks

---

## Data Augmentation

- Generate synthetic data to augment training sets
- Applications:
  - Addressing class imbalance
  - Creating larger datasets for model training
- GPT models can generate realistic synthetic data


---


## R Example: Text Generation (Simple Markov Chain)

.midi[
```{r}
library(tidytext)
library(dplyr)
library(stringr)

# Sample text
text <- "The quick brown fox jumps over the lazy dog. The dog barks at the fox. The fox runs away quickly."

# Tokenize and create word pairs
word_pairs <- tibble(text = text) %>%
  unnest_tokens(word, text) %>%
  mutate(next_word = lead(word)) %>%
  na.omit()

word_pairs
```
]

---

## Create Markov Chain

.midi[
```{r}
# Create a simple Markov chain
markov_chain <- word_pairs %>%
  group_by(word) %>%
  summarize(next_words = list(next_word))

markov_chain
```
]

---

## Text Generation Function

.midi[
```{r}
generate_text <- function(start_word, length = 10) {
  result <- start_word
  current_word <- start_word
  
  for (i in 1:length) {
    next_word_options <- markov_chain %>%
      filter(word == current_word) %>%
      pull(next_words) %>%
      unlist()
    
    if (length(next_word_options) == 0) break
    
    next_word <- sample(next_word_options, 1)
    result <- c(result, next_word)
    current_word <- next_word
  }
  
  str_c(result, collapse = " ")
}
```
]

---

## Generate Text

.midi[
```{r}
# Generate a sentence
cat(generate_text("the", 8))
```
]


---

class: middle

# 3. Question Answering and Information Retrieval

---


## Text-to-SQL

- Generate SQL queries from natural language
- Use cases:
  - Database querying for non-technical users
  - Automating data analysis workflows
- GPT-3 and other LLMs can be fine-tuned for this task

---



## Question Answering

- Extract relevant answers from a given context
- Applications:
  - Automated customer support
  - Information retrieval systems  
- Models like RoBERTa excel at question answering tasks

---

## Question Answering Systems
.midi[
```{r}
library(tidytext)
library(dplyr)

context <- "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. 
It is named after the engineer Gustave Eiffel, whose company designed and built the Tower."

question <- "Who designed the Eiffel Tower?"

# Tokenize context and question
context_tokens <- tibble(text = context) %>%
  unnest_tokens(word, text)

question_tokens <- tibble(text = question) %>%
  unnest_tokens(word, text)

# Simple word overlap for demonstration
matching_words <- intersect(context_tokens$word, question_tokens$word)

# Find sentence with most matching words
sentences <- tibble(text = context) %>%
  unnest_tokens(sentence, text, token = "sentences")

best_sentence <- sentences %>%
  mutate(matches = sapply(sentence, function(s) {
    sum(matching_words %in% unlist(strsplit(s, " ")))
  })) %>%
  arrange(desc(matches)) %>%
  slice(1)

print(best_sentence$sentence)
```
]

---

class: middle

# Question Answering with R: A Breakdown

---

## Step 1: Setup

.pull-left-wide[.midi[
```{r}
library(tidytext)
library(dplyr)

context <- "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower."

question <- "Who designed the Eiffel Tower?"
```
]]
.pull-right-narrow[
- Load necessary libraries
- Define the context (text containing the answer)
- Define the question we want to answer
]

---

## Step 2: Tokenization

.pull-left.midi[
```{r}
context_tokens <- tibble(text = context) %>%
  unnest_tokens(word, text)

(question_tokens <- tibble(text = question) %>%
  unnest_tokens(word, text))
```
]
.pull-right[
- Break down context and question into individual words (tokens)
- `unnest_tokens()` from `tidytext` package does the heavy lifting
- Result: Two data frames with one word per row
]

---

## Step 3: Word Matching

.pull-left[
```{r}
matching_words <- intersect(
  context_tokens$word, 
  question_tokens$word
)
```
]
.pull-right[
- Find words that appear in both context and question
- `intersect()` function identifies common words
- This helps focus on potentially relevant parts of the context
]

---

## Step 4: Sentence Splitting


```{r}
sentences <- tibble(text = context) %>%
  unnest_tokens(sentence,
                text,
                token = "sentences")
```

--

- Split the context into individual sentences
- Again using `unnest_tokens()`, but with `token = "sentences"`
- Prepares for sentence-level analysis



---

## Step 5: Answer Extraction (Part 1)


```{r}
best_sentence <- sentences %>%
  mutate(matches = sapply(sentence, function(s) {
    sum(matching_words %in% unlist(strsplit(s, " ")))
  }))
```

--

- Count matching words in each sentence
- `sapply()` applies the counting function to each sentence
- Creates a new column 'matches' with the count


---

## Step 5: Answer Extraction (Part 2)

.pull-left[
```{r}
best_sentence <- best_sentence %>%
  arrange(desc(matches)) %>%
  slice(1)
```
]

.pull-right[
- Rank sentences by number of matching words
- `arrange(desc(matches))` sorts in descending order
- `slice(1)` selects the top-ranked sentence
]

---

## Step 6: Result Output


.pull-left[
- Print the sentence most likely to contain the answer
- This sentence has the most word overlap with the question
- A simple but effective approach for basic question answering
]


.pull-right[
```{r}
print(best_sentence$sentence)
```
]



---

class: middle

# Debrief

- This method demonstrates a basic approach to question answering in R
- It relies on word overlap between question and context
- More advanced methods would involve semantic understanding and machine learning models

.question[
How might you improve this basic question answering system?
]

---

# Conclusion

- LLMs have diverse applications across data science tasks
- They excel at generating human language
- Continued research is improving their capabilities and efficiency
- R provides tools for implementing some of these techniques

---

class: middle

# Summary: Learning Goals Achieved

---

## What We've Learned

Today, you should now be able to...

.pull-left[
### Applications
- ✅ Text classification and sentiment analysis
- ✅ Named entity recognition
- ✅ Text generation techniques
]

.pull-right[
### Skills
- ✅ Build QA systems
- ✅ Implement NLP in R
- ✅ Evaluate LLM techniques
]

---

class: middle

# Wrapping Up...


