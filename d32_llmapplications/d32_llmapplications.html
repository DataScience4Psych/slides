<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Use Cases of Large Language Models   ðŸ¤–</title>
    <meta charset="utf-8" />
    <meta name="author" content="S. Mason Garrison" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.min.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="../slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Use Cases of Large Language Models <br> ðŸ¤–
]
.author[
### S. Mason Garrison
]

---





layout: true
  
&lt;div class="my-footer"&gt;
&lt;span&gt;
&lt;a href="https://DataScience4Psych.github.io/DataScience4Psych/" target="_blank"&gt;Data Science for Psychologists&lt;/a&gt;
&lt;/span&gt;
&lt;/div&gt; 

---





class: middle

# Use Cases of Large Language Models

---

# Roadmap

1. Text Understanding and Analysis
2. Text Generation and Transformation
3. Question Answering and Information Retrieval


---

class: middle

# 1. Text Understanding and Analysis

---

## Text Understanding and Analysis

- Extract insights from text data
- Applications:
  - Text classification
  - Named entity recognition
  - Text similarity and clustering
  - Anomaly detection

---

## Text Classification 

- Categorize text into predefined classes
- Applications:
  - Sentiment analysis 
  - Topic classification
  - Intent detection in chatbots
- Fine-tuned LLMs like BERT achieve state-of-the-art results
  - [Tensorflow has really solid tutorials](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)

---

## Named Entity Recognition

- Identify and classify named entities in within unstructured text.
- Use cases:
  - Information extraction
  - Building knowledge graphs
  - Enhancing search capabilities
- Models like [SpaCy](https://spacy.io/) and [BERT](https://arxiv.org/abs/1810.04805) can be fine-tuned for NER

---

## Text Similarity and Clustering

- Measure semantic similarity between texts
- Use cases:
  - Document de-duplication
  - Content recommendation systems
- Sentence transformers like [SBERT](https://sbert.net/) are effective for this

---

## Anomaly Detection in Text Data

- Identify unusual patterns or outliers in text
- Use cases:
  - Fraud detection in financial documents
  - Identifying errors in medical records
- Autoencoder architectures with LLMs can be used
- **Check out this [Paper that talks more about this](https://arxiv.org/abs/2211.13900)**

---

## R Example: Text Classification (Sentiment Analysis)

.midi[

``` r
library(tidytext)
library(dplyr)
library(ggplot2)

# Sample text data
texts &lt;- c(
  "I love this product! It's amazing.",
  "This is terrible. I hate it.",
  "It's okay, nothing special.",
  "Wow, absolutely fantastic experience!",
  "Disappointed with the quality."
)

# Create a tibble
df &lt;- tibble(text = texts)
```
]

---

## Tokenize and Perform Sentiment Analysis

.midi[

``` r
sentiment_scores &lt;- df %&gt;%
  unnest_tokens(word, text) %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment) %&gt;%
  spread(sentiment, n, fill = 0) %&gt;%
  mutate(sentiment_score = positive - negative)

sentiment_scores
```

```
## # A tibble: 1 Ã— 3
##   negative positive sentiment_score
##      &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;
## 1        3        4               1
```
]

---

## Visualize Sentiment Analysis Results

.midi[
&lt;img src="d32_llmapplications_files/figure-html/unnamed-chunk-4-1.png" width="60%" style="display: block; margin: auto;" /&gt;
]

---

class: middle

# 2. Text Generation and Transformation

---

## Text Generation

.pull-left[
- LLMs can generate human-like text on any topic
- Applications:
  - Automated content creation 
  - Chatbots and conversational AI
  - Code generation
- Examples: 
  - GPT-3 for natural language generation
  - GitHub Copilot for code completion
]

.pull-right[
&lt;img src="img/An image illustrating automated content creation.png" width="90%" style="display: block; margin: auto;" /&gt;
]

---

## Summarization

.pull-left[
- Condense long documents into brief summaries
- Use cases:
  - Summarizing research papers
  - Creating article abstracts
  - Distilling key points from large datasets
- Models like BART and T5 specialize in summarization tasks
]

---

## Machine Translation

- Translate text between languages
- Applications:
  - Localization of content
  - Cross-lingual information retrieval
- Models like mT5 specialize in multilingual tasks

---

## Data Augmentation

- Generate synthetic data to augment training sets
- Applications:
  - Addressing class imbalance
  - Creating larger datasets for model training
- GPT models can generate realistic synthetic data


---


## R Example: Text Generation (Simple Markov Chain)

.tiny[

``` r
library(tidytext)
library(dplyr)
library(stringr)

# Sample text
text &lt;- "The quick brown fox jumps over the lazy dog. The dog barks at the fox. The fox runs away quickly."

# Tokenize and create word pairs
word_pairs &lt;- tibble(text = text) %&gt;%
  unnest_tokens(word, text) %&gt;%
  mutate(next_word = lead(word)) %&gt;%
  na.omit()

word_pairs
```

```
## # A tibble: 19 Ã— 2
##   word  next_word
##   &lt;chr&gt; &lt;chr&gt;    
## 1 the   quick    
## 2 quick brown    
## 3 brown fox      
## 4 fox   jumps    
## 5 jumps over     
## 6 over  the      
## # â„¹ 13 more rows
```
]

---

## Create Markov Chain

.midi[

``` r
# Create a simple Markov chain
markov_chain &lt;- word_pairs %&gt;%
  group_by(word) %&gt;%
  summarise(next_words = list(next_word))

markov_chain
```

```
## # A tibble: 12 Ã— 2
##   word  next_words
##   &lt;chr&gt; &lt;list&gt;    
## 1 at    &lt;chr [1]&gt; 
## 2 away  &lt;chr [1]&gt; 
## 3 barks &lt;chr [1]&gt; 
## 4 brown &lt;chr [1]&gt; 
## 5 dog   &lt;chr [2]&gt; 
## 6 fox   &lt;chr [3]&gt; 
## # â„¹ 6 more rows
```
]

---

## Text Generation Function

.midi[

``` r
generate_text &lt;- function(start_word, length = 10) {
  result &lt;- start_word
  current_word &lt;- start_word
  
  for (i in 1:length) {
    next_word_options &lt;- markov_chain %&gt;%
      filter(word == current_word) %&gt;%
      pull(next_words) %&gt;%
      unlist()
    
    if (length(next_word_options) == 0) break
    
    next_word &lt;- sample(next_word_options, 1)
    result &lt;- c(result, next_word)
    current_word &lt;- next_word
  }
  
  str_c(result, collapse = " ")
}
```
]

---

## Generate Text

.midi[

``` r
# Generate a sentence
cat(generate_text("the", 8))
```

```
## the fox the fox runs away quickly
```
]


---

class: middle

# 3. Question Answering and Information Retrieval

---


## Text-to-SQL

- Generate SQL queries from natural language
- Use cases:
  - Database querying for non-technical users
  - Automating data analysis workflows
- GPT-3 and other LLMs can be fine-tuned for this task

---



## Question Answering

- Extract relevant answers from a given context
- Applications:
  - Automated customer support
  - Information retrieval systems  
- Models like RoBERTa excel at question answering tasks

---

## Question Answering Systems
.midi[

``` r
library(tidytext)
library(dplyr)

context &lt;- "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. 
It is named after the engineer Gustave Eiffel, whose company designed and built the Tower."

question &lt;- "Who designed the Eiffel Tower?"

# Tokenize context and question
context_tokens &lt;- tibble(text = context) %&gt;%
  unnest_tokens(word, text)

question_tokens &lt;- tibble(text = question) %&gt;%
  unnest_tokens(word, text)

# Simple word overlap for demonstration
matching_words &lt;- intersect(context_tokens$word, question_tokens$word)

# Find sentence with most matching words
sentences &lt;- tibble(text = context) %&gt;%
  unnest_tokens(sentence, text, token = "sentences")

best_sentence &lt;- sentences %&gt;%
  mutate(matches = sapply(sentence, function(s) {
    sum(matching_words %in% unlist(strsplit(s, " ")))
  })) %&gt;%
  arrange(desc(matches)) %&gt;%
  slice(1)

print(best_sentence$sentence)
```

```
## [1] "the eiffel tower is a wrought-iron lattice tower on the champ de mars in paris, france."
```
]

---

class: middle

# Question Answering with R: A Breakdown

---

## Step 1: Setup

.pull-left-wide[.midi[

``` r
library(tidytext)
library(dplyr)

context &lt;- "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower."

question &lt;- "Who designed the Eiffel Tower?"
```
]]
.pull-right-narrow[
- Load necessary libraries
- Define the context (text containing the answer)
- Define the question we want to answer
]

---

## Step 2: Tokenization

.pull-left.midi[

``` r
context_tokens &lt;- tibble(text = context) %&gt;%
  unnest_tokens(word, text)

(question_tokens &lt;- tibble(text = question) %&gt;%
  unnest_tokens(word, text))
```

```
## # A tibble: 5 Ã— 1
##   word    
##   &lt;chr&gt;   
## 1 who     
## 2 designed
## 3 the     
## 4 eiffel  
## 5 tower
```
]
.pull-right[
- Break down context and question into individual words (tokens)
- `unnest_tokens()` from `tidytext` package does the heavy lifting
- Result: Two data frames with one word per row
]

---

## Step 3: Word Matching

.pull-left[

``` r
matching_words &lt;- intersect(
  context_tokens$word, 
  question_tokens$word
)
```
]
.pull-right[
- Find words that appear in both context and question
- `intersect()` function identifies common words
- This helps focus on potentially relevant parts of the context
]

---

## Step 4: Sentence Splitting



``` r
sentences &lt;- tibble(text = context) %&gt;%
  unnest_tokens(sentence,
                text,
                token = "sentences")
```

--

- Split the context into individual sentences
- Again using `unnest_tokens()`, but with `token = "sentences"`
- Prepares for sentence-level analysis



---

## Step 5: Answer Extraction (Part 1)



``` r
best_sentence &lt;- sentences %&gt;%
  mutate(matches = sapply(sentence, function(s) {
    sum(matching_words %in% unlist(strsplit(s, " ")))
  }))
```

--

- Count matching words in each sentence
- `sapply()` applies the counting function to each sentence
- Creates a new column 'matches' with the count


---

## Step 5: Answer Extraction (Part 2)

.pull-left[

``` r
best_sentence &lt;- best_sentence %&gt;%
  arrange(desc(matches)) %&gt;%
  slice(1)
```
]

.pull-right[
- Rank sentences by number of matching words
- `arrange(desc(matches))` sorts in descending order
- `slice(1)` selects the top-ranked sentence
]

---

## Step 6: Result Output


.pull-left[
- Print the sentence most likely to contain the answer
- This sentence has the most word overlap with the question
- A simple but effective approach for basic question answering
]


.pull-right[

``` r
print(best_sentence$sentence)
```

```
## [1] "the eiffel tower is a wrought-iron lattice tower on the champ de mars in paris, france."
```
]



---

class: middle

# Debrief

- This method demonstrates a basic approach to question answering in R
- It relies on word overlap between question and context
- More advanced methods would involve semantic understanding and machine learning models

.question[
How might you improve this basic question answering system?
]

---

# Conclusion

- LLMs have diverse applications across data science tasks
- They excel at understanding and generating human language
- Continued research is improving their capabilities and efficiency
- R provides tools for implementing some of these techniques

---

class: middle

# Wrapping Up...


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLines": true,
"highlightStyle": "solarized-light",
"countIncrementalSlides": false,
"slideNumberFormat": ""
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
